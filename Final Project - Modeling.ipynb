{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('phillies_2019.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    phillies_2019 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('phils_prior.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    phils_prior = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    prediction = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction1.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    prediction1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine and Re-Organize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum = pd.concat([phils_prior, phillies_2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.reset_index(drop=True, inplace = True)\n",
    "\n",
    "Y = phils_total_sum.opp_Win\n",
    "\n",
    "phils_total_sum = phils_total_sum.drop(['opp_Win'], axis=1)\n",
    "phils_total_sum = phils_total_sum.drop(['Game', 'opp_Game', 'IP', 'opp_IP'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_columns = phils_total_sum.columns[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in int_columns:\n",
    "    phils_total_sum[x] = phils_total_sum[x].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=12, column = 'BA', value = phils_total_sum['H']/phils_total_sum['AB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=34, column = 'opp_BA', value = phils_total_sum['opp_H']/phils_total_sum['opp_AB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=13, column = 'OBP', value = (phils_total_sum['H'] + phils_total_sum['BB'] + phils_total_sum['opp_HBP'])/ phils_total_sum['PA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=36, column = 'opp_OBP', value = (phils_total_sum['opp_H'] + phils_total_sum['opp_BB'] + phils_total_sum['HBP'])/ phils_total_sum['opp_PA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=14, column = 'SLG', value = (((phils_total_sum['H']-(phils_total_sum['2B']+phils_total_sum['3B']+phils_total_sum['HR']))+\n",
    "                                                        (phils_total_sum['2B']*2) + (phils_total_sum['3B'] *3) + (phils_total_sum['HR']*4))/ \n",
    "                                                        phils_total_sum['AB']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=38, column = 'opp_SLG', value = (((phils_total_sum['opp_H']-(phils_total_sum['opp_2B']+phils_total_sum['opp_3B']+phils_total_sum['opp_HR']))+\n",
    "                                                        (phils_total_sum['opp_2B']*2) + (phils_total_sum['opp_3B'] *3) + (phils_total_sum['opp_HR']*4))/ \n",
    "                                                        phils_total_sum['opp_AB']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=15, column = 'OPS', value = phils_total_sum['OBP'] + phils_total_sum['SLG'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=40, column = 'opp_OPS', value = phils_total_sum['opp_OBP'] + phils_total_sum['opp_SLG'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(754, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_team = [col for col in phils_total_sum.columns if phils_total_sum[col].dtype in [np.object]]\n",
    "X_team = phils_total_sum[features_team]\n",
    "\n",
    "np.shape(X_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummies\n",
    "X_team = pd.get_dummies(X_team, drop_first=True)\n",
    "X_team = X_team.loc[1:,:]\n",
    "X_team.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6566927b6b43e39b378101d8a9b3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=753), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "averages = pd.DataFrame(columns = phils_total_sum.columns[:-2])\n",
    "\n",
    "for i in tqdm(range(1,len(phils_total_sum))):\n",
    "    if i <=15:\n",
    "        temp = phils_total_sum.loc[:i, :].mean()\n",
    "        temp = pd.DataFrame(temp).T\n",
    "        temp['opp_Win'] = Y[i]\n",
    "        averages = pd.concat([averages,temp], ignore_index=True)\n",
    "    elif i > 15:\n",
    "        temp = phils_total_sum.loc[(i-3):i, :].mean()\n",
    "        temp = pd.DataFrame(temp).T\n",
    "        temp['opp_Win'] = Y[i]\n",
    "        averages = pd.concat([averages,temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = averages.drop(columns = ('opp_Win'))\n",
    "Y = averages['opp_Win']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.astype(int)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "# model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    416\n",
      "1.0    148\n",
      "Name: opp_Win, dtype: int64\n",
      "0.0    0.737589\n",
      "1.0    0.262411\n",
      "Name: opp_Win, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros.\n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r^2: 0.475177304964539\n",
      "Testing r^2: 0.455026455026455\n",
      "Training MSE: 0.524822695035461\n",
      "Testing MSE: 0.544973544973545\n"
     ]
    }
   ],
   "source": [
    "print('Training r^2:', logreg.score(X_train.astype(int), y_train.astype(int)))\n",
    "print('Testing r^2:', logreg.score(X_test.astype(int), y_test.astype(int)))\n",
    "print('Training MSE:', mean_squared_error(y_train.astype(int), logreg.predict(X_train.astype(int))))\n",
    "print('Testing MSE:', mean_squared_error(y_test.astype(int), logreg.predict(X_test.astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# scale the data and perform train test split\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r^2: 0.7127659574468085\n",
      "Testing r^2: 0.6666666666666666\n",
      "Training MSE: 0.2872340425531915\n",
      "Testing MSE: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(fit_intercept = False, C = 1e0, solver='liblinear')\n",
    "model_log = logreg.fit(X_train.astype(int), y_train.astype(int))\n",
    "print('Training r^2:', logreg.score(X_train.astype(int), y_train.astype(int)))\n",
    "print('Testing r^2:', logreg.score(X_test.astype(int), y_test.astype(int)))\n",
    "print('Training MSE:', mean_squared_error(y_train.astype(int), logreg.predict(X_train.astype(int))))\n",
    "print('Testing MSE:', mean_squared_error(y_test.astype(int), logreg.predict(X_test.astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pd.concat([pd.DataFrame(X_scaled), X_team], axis = 1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled,Y)\n",
    "X_train = X_scaled[:600,:]\n",
    "X_test = X_scaled[600:, :]\n",
    "y_train = Y.loc[:599]\n",
    "y_test = Y.loc[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r^2: 0.7233333333333334\n",
      "Testing r^2: 0.6601307189542484\n",
      "Training MSE: 0.27666666666666667\n",
      "Testing MSE: 0.33986928104575165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg_all = LogisticRegression(fit_intercept = True, C = 1e0, penalty='l1')\n",
    "logreg_all.fit(X_train.astype(int), y_train.astype(int))\n",
    "print('Training r^2:', logreg_all.score(X_train.astype(int), y_train.astype(int)))\n",
    "print('Testing r^2:', logreg_all.score(X_test.astype(int), y_test.astype(int)))\n",
    "print('Training MSE:', mean_squared_error(y_train.astype(int), logreg_all.predict(X_train.astype(int))))\n",
    "print('Testing MSE:', mean_squared_error(y_test.astype(int), logreg_all.predict(X_test.astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = phils_total_sum.corr()\n",
    "# corr[corr<-0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.6216666666666667\n",
      "Testing Accuracy:  0.6339869281045751\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def precision(y_hat, y):\n",
    "    #Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0]==1 and i[1]==1])\n",
    "    fp = sum([1 for i in y_y_hat if i[0]==0 and i[1]==1])\n",
    "    return tp/float(tp+fp)\n",
    "\n",
    "def recall(y_hat, y):\n",
    "    #Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0]==1 and i[1]==1])\n",
    "    fn = sum([1 for i in y_y_hat if i[0]==1 and i[1]==0])\n",
    "    return tp/float(tp+fn)\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    #Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0]==1 and i[1]==1])\n",
    "    tn = sum([1 for i in y_y_hat if i[0]==0 and i[1]==0])\n",
    "    return (tp+tn)/float(len(y_hat))\n",
    "\n",
    "def f1(y_hat,y):\n",
    "    precision_score = precision(y_hat,y)\n",
    "    recall_score = recall(y_hat,y)\n",
    "    numerator = precision_score * recall_score\n",
    "    denominator = precision_score + recall_score\n",
    "    return 2 * (numerator / denominator)\n",
    "\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "\n",
    "#print('Training Precision: ', precision(y_hat_train, y_train))\n",
    "#print('Testing Precision: ', precision(y_hat_test, y_test))\n",
    "#print('\\n\\n')\n",
    "\n",
    "#print('Training Recall: ', recall(y_hat_train, y_train))\n",
    "#print('Testing Recall: ', recall(y_hat_test, y_test))\n",
    "#print('\\n\\n')\n",
    "\n",
    "print('Training Accuracy: ', accuracy(y_hat_train, y_train))\n",
    "print('Testing Accuracy: ', accuracy(y_hat_test, y_test))\n",
    "print('\\n\\n')\n",
    "\n",
    "#print('Training F1-Score: ',f1(y_hat_train,y_train))\n",
    "#print('Testing F1-Score: ',f1(y_hat_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = Y\n",
    "#labels_removed_df = X\n",
    "#scaler = StandardScaler()\n",
    "#scaled_df = scaler.fit_transform(labels_removed_df)\n",
    "X = averages.drop(columns = ('opp_Win'))\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# X_all = pd.concat([pd.DataFrame(X_scaled), X_team], axis = 1)\n",
    "X_train = X_scaled[:600,:]\n",
    "X_test = X_scaled[600:, :]\n",
    "y_train = Y.loc[:599]\n",
    "y_test = Y.loc[600:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 50)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_odds(game):\n",
    "    prob_phils_win = game[1]\n",
    "    prob_other_win = game[0]\n",
    "    if prob_phils_win >0.5:\n",
    "        x = ((-prob_phils_win)*100)/(1-prob_phils_win)\n",
    "        y = (100-(prob_other_win*100))/prob_other_win\n",
    "    else:\n",
    "        x = (100-(prob_phils_win*100))/prob_phils_win\n",
    "        y = ((-prob_other_win)*100)/(1-prob_other_win)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 73.33%\n",
      "Validation accuracy: 65.36%\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth = 1, min_child_weight = 4)\n",
    "clf.fit(X_train, y_train)\n",
    "training_preds = clf.predict(X_train)\n",
    "val_preds = clf.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train.values.astype(int), training_preds.astype(int))\n",
    "val_accuracy = accuracy_score(y_test.values.astype(int), val_preds.astype(int))\n",
    "\n",
    "print(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\n",
    "print(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction1.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    prediction1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_proba = clf.predict_proba(prediction1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40942627, 0.5905737 ], dtype=float32)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_proba[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-144.24421894610202, 144.24421894610202)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_odds(prediction_proba[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds2 = pd.DataFrame(prob_odds(prediction_proba[0])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds2.rename(columns = {0:'Phillies Odds', 1:'Opponent Odds'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds2['Date'] = ''\n",
    "Historical_odds2['Phillies Westgate Odds'] = ''\n",
    "Historical_odds2['Opponent Westgate Odds'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds2['Date'][0] = 'July 18, 2019'\n",
    "Historical_odds2['Phillies Westgate Odds'][0] = -103\n",
    "Historical_odds2['Opponent Westgate Odds'][0] = -107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds = pd.concat([Historical_odds, Historical_odds2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds['Opponent'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds['Opponent'][4] = 'Dodgers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds.to_html('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('BG_predictions.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(Historical_odds, f, pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds['Phillies Actual Odds'][0] = -130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds['Opponent Actual Odds'][0] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds['3'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds['4'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds.rename(columns ={'Phillies Actual Odds': 'Phillies Westgate Odds', 'Opponent Actual Odds': 'Opponent Westgate Odds'}, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Historical_odds.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = clf.predict_proba(X_test[0,:].reshape(1,-1)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100 - game[0][0]*100) / game[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(12,20):\n",
    "    game = clf.predict_proba(X_test[-x,:].reshape(1,-1)).tolist()[0]\n",
    "    print(prob_odds(game))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
