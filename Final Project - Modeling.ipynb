{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('phillies_2019.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    phillies_2019 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('phils_prior.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    phils_prior = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine and Re-Organize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum = pd.concat([phils_prior, phillies_2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.reset_index(drop=True, inplace = True)\n",
    "\n",
    "Y = phils_total_sum.opp_Win\n",
    "\n",
    "phils_total_sum = phils_total_sum.drop(['opp_Win'], axis=1)\n",
    "phils_total_sum = phils_total_sum.drop(['Game', 'opp_Game', 'IP', 'opp_IP'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_columns = phils_total_sum.columns[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in int_columns:\n",
    "    phils_total_sum[x] = phils_total_sum[x].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=12, column = 'BA', value = phils_total_sum['H']/phils_total_sum['AB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=34, column = 'opp_BA', value = phils_total_sum['opp_H']/phils_total_sum['opp_AB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=13, column = 'OBP', value = (phils_total_sum['H'] + phils_total_sum['BB'] + phils_total_sum['opp_HBP'])/ phils_total_sum['PA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=36, column = 'opp_OBP', value = (phils_total_sum['opp_H'] + phils_total_sum['opp_BB'] + phils_total_sum['HBP'])/ phils_total_sum['opp_PA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=14, column = 'SLG', value = (((phils_total_sum['H']-(phils_total_sum['2B']+phils_total_sum['3B']+phils_total_sum['HR']))+\n",
    "                                                        (phils_total_sum['2B']*2) + (phils_total_sum['3B'] *3) + (phils_total_sum['HR']*4))/ \n",
    "                                                        phils_total_sum['AB']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=38, column = 'opp_SLG', value = (((phils_total_sum['opp_H']-(phils_total_sum['opp_2B']+phils_total_sum['opp_3B']+phils_total_sum['opp_HR']))+\n",
    "                                                        (phils_total_sum['opp_2B']*2) + (phils_total_sum['opp_3B'] *3) + (phils_total_sum['opp_HR']*4))/ \n",
    "                                                        phils_total_sum['opp_AB']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=15, column = 'OPS', value = phils_total_sum['OBP'] + phils_total_sum['SLG'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "phils_total_sum.insert(loc=40, column = 'opp_OPS', value = phils_total_sum['opp_OBP'] + phils_total_sum['opp_SLG'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_team = [col for col in phils_total_sum.columns if phils_total_sum[col].dtype in [np.object]]\n",
    "X_team = phils_total_sum[features_team]\n",
    "\n",
    "np.shape(X_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753, 54)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make dummies\n",
    "X_team = pd.get_dummies(X_team, drop_first=True)\n",
    "np.shape(X_team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = pd.DataFrame(columns = phils_total_sum.columns[:-2])\n",
    "\n",
    "for i in range(1,len(phils_total_sum)):\n",
    "    temp = phils_total_sum.loc[:i, :].mean()\n",
    "    temp = pd.DataFrame(temp).T\n",
    "    temp['opp_Win'] = Y[i]\n",
    "    averages = pd.concat([averages,temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = averages.drop(columns = ('opp_Win'))\n",
    "Y = averages['opp_Win']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.astype(int)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "# model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    357\n",
      "1.0    207\n",
      "Name: opp_Win, dtype: int64\n",
      "0.0    0.632979\n",
      "1.0    0.367021\n",
      "Name: opp_Win, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros.\n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r^2: 0.49645390070921985\n",
      "Testing r^2: 0.5851063829787234\n",
      "Training MSE: 0.5035460992907801\n",
      "Testing MSE: 0.4148936170212766\n"
     ]
    }
   ],
   "source": [
    "print('Training r^2:', logreg.score(X_train.astype(int), y_train.astype(int)))\n",
    "print('Testing r^2:', logreg.score(X_test.astype(int), y_test.astype(int)))\n",
    "print('Training MSE:', mean_squared_error(y_train.astype(int), logreg.predict(X_train.astype(int))))\n",
    "print('Testing MSE:', mean_squared_error(y_test.astype(int), logreg.predict(X_test.astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# scale the data and perform train test split\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r^2: 0.5921985815602837\n",
      "Testing r^2: 0.5159574468085106\n",
      "Training MSE: 0.4078014184397163\n",
      "Testing MSE: 0.48404255319148937\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(fit_intercept = False, C = 1e0, solver='liblinear')\n",
    "model_log = logreg.fit(X_train.astype(int), y_train.astype(int))\n",
    "print('Training r^2:', logreg.score(X_train.astype(int), y_train.astype(int)))\n",
    "print('Testing r^2:', logreg.score(X_test.astype(int), y_test.astype(int)))\n",
    "print('Training MSE:', mean_squared_error(y_train.astype(int), logreg.predict(X_train.astype(int))))\n",
    "print('Testing MSE:', mean_squared_error(y_test.astype(int), logreg.predict(X_test.astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pd.concat([pd.DataFrame(X_scaled), X_team], axis = 1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled,Y)\n",
    "X_train = X_scaled[:600,:]\n",
    "X_test = X_scaled[600:, :]\n",
    "y_train = Y.loc[:599]\n",
    "y_test = Y.loc[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training r^2: 0.5933333333333334\n",
      "Testing r^2: 0.45394736842105265\n",
      "Training MSE: 0.4066666666666667\n",
      "Testing MSE: 0.5460526315789473\n"
     ]
    }
   ],
   "source": [
    "logreg_all = LogisticRegression(fit_intercept = True, C = 1e0, penalty='l1')\n",
    "logreg_all.fit(X_train.astype(int), y_train.astype(int))\n",
    "print('Training r^2:', logreg_all.score(X_train.astype(int), y_train.astype(int)))\n",
    "print('Testing r^2:', logreg_all.score(X_test.astype(int), y_test.astype(int)))\n",
    "print('Training MSE:', mean_squared_error(y_train.astype(int), logreg_all.predict(X_train.astype(int))))\n",
    "print('Testing MSE:', mean_squared_error(y_test.astype(int), logreg_all.predict(X_test.astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = phils_total_sum.corr()\n",
    "# corr[corr<-0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.5633333333333334\n",
      "Testing Accuracy:  0.46710526315789475\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def precision(y_hat, y):\n",
    "    #Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0]==1 and i[1]==1])\n",
    "    fp = sum([1 for i in y_y_hat if i[0]==0 and i[1]==1])\n",
    "    return tp/float(tp+fp)\n",
    "\n",
    "def recall(y_hat, y):\n",
    "    #Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0]==1 and i[1]==1])\n",
    "    fn = sum([1 for i in y_y_hat if i[0]==1 and i[1]==0])\n",
    "    return tp/float(tp+fn)\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    #Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0]==1 and i[1]==1])\n",
    "    tn = sum([1 for i in y_y_hat if i[0]==0 and i[1]==0])\n",
    "    return (tp+tn)/float(len(y_hat))\n",
    "\n",
    "def f1(y_hat,y):\n",
    "    precision_score = precision(y_hat,y)\n",
    "    recall_score = recall(y_hat,y)\n",
    "    numerator = precision_score * recall_score\n",
    "    denominator = precision_score + recall_score\n",
    "    return 2 * (numerator / denominator)\n",
    "\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "\n",
    "#print('Training Precision: ', precision(y_hat_train, y_train))\n",
    "#print('Testing Precision: ', precision(y_hat_test, y_test))\n",
    "#print('\\n\\n')\n",
    "\n",
    "#print('Training Recall: ', recall(y_hat_train, y_train))\n",
    "#print('Testing Recall: ', recall(y_hat_test, y_test))\n",
    "#print('\\n\\n')\n",
    "\n",
    "print('Training Accuracy: ', accuracy(y_hat_train, y_train))\n",
    "print('Testing Accuracy: ', accuracy(y_hat_test, y_test))\n",
    "print('\\n\\n')\n",
    "\n",
    "#print('Training F1-Score: ',f1(y_hat_train,y_train))\n",
    "#print('Testing F1-Score: ',f1(y_hat_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = Y\n",
    "#labels_removed_df = X\n",
    "#scaler = StandardScaler()\n",
    "#scaled_df = scaler.fit_transform(labels_removed_df)\n",
    "X = averages.drop(columns = ('opp_Win'))\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "X_train = X_scaled[:600,:]\n",
    "X_test = X_scaled[600:, :]\n",
    "y_train = Y.loc[:599]\n",
    "y_test = Y.loc[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((152, 50), (152,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.0%\n",
      "Validation accuracy: 56.58%\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth = 7)\n",
    "clf.fit(X_train, y_train)\n",
    "training_preds = clf.predict(X_train)\n",
    "val_preds = clf.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train.values.astype(int), training_preds.astype(int))\n",
    "val_accuracy = accuracy_score(y_test.values.astype(int), val_preds.astype(int))\n",
    "\n",
    "print(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\n",
    "print(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_odds(game):\n",
    "    prob_phils_win = game[0]\n",
    "    prob_other_win = game[1]\n",
    "    if prob_phils_win >0.5:\n",
    "        x = ((-prob_phils_win)*100)/(1-prob_phils_win)\n",
    "        y = (100-(prob_other_win*100))/prob_other_win\n",
    "    else:\n",
    "        x = (100-(prob_phils_win*100))/prob_phils_win\n",
    "        y = ((-prob_other_win)*100)/(1-prob_other_win)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = clf.predict_proba(X_test[0,:].reshape(1,-1)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185.38154938566447"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 - game[0][0]*100) / game[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-155.083449543784, 155.08346893543134)\n",
      "(-178.57662576766575, 178.576648895741)\n",
      "(-119.35348233715335, 119.35348233715335)\n",
      "(172.55938857286284, -172.55938857286284)\n",
      "(164.68530983101505, -164.68530983101505)\n",
      "(-167.97154366360957, 167.97156506428593)\n",
      "(-156.12928400749158, 156.12926445651092)\n",
      "(-127.37810145456685, 127.3780860465285)\n"
     ]
    }
   ],
   "source": [
    "for x in range(12,20):\n",
    "    game = clf.predict_proba(X_test[-x,:].reshape(1,-1)).tolist()[0]\n",
    "    print(prob_odds(game))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
